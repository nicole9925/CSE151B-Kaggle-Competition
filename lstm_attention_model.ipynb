{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import RandomSampler, random_split, SubsetRandomSampler, WeightedRandomSampler\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"./new_train/new_train\"\n",
    "val_path = \"./new_val_in/new_val_in\" \n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "train_dataset  = ArgoverseDataset(data_path=new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 32\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    out = [numpy.dstack([scene['p_out'], scene['v_out']]) for scene in batch]\n",
    "    out = torch.LongTensor(out)\n",
    "    inp = torch.tensor(inp, dtype=torch.float)\n",
    "    out = torch.tensor(out, dtype=torch.float)\n",
    "    return [inp, out]\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, (199078, 6864))\n",
    "\n",
    "sampler = RandomSampler(train_dataset, num_samples = 50000, replacement=True)\n",
    "# sampler = RandomSampler(train_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0, sampler = sampler, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "        self.fc2 = nn.Linear(output_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10000):\n",
    "    model.train()\n",
    "    iterator = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    loss_ema = -1\n",
    "    for batch_idx, (data, target) in enumerate(iterator):\n",
    "        data, target = torch.reshape(data, (batch_sz, 60, -1)).to(device), torch.reshape(target[:,:,:,:2], (batch_sz, 60, -1)).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss_ema < 0:\n",
    "            loss_ema = loss.item()\n",
    "        loss_ema = loss_ema * 0.99 + loss.item()*0.01\n",
    "        iterator.set_postfix(loss=loss_ema)\n",
    "    return running_loss\n",
    "#         iterator.set_postfix(loss=(loss.item()*data.size(0) / (counter * train_loader.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    loss_ema = -1\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = torch.reshape(data, (batch_sz, 60, -1)).to(device), torch.reshape(target[:,:,:,:2], (batch_sz, 60, -1)).to(device)\n",
    "            output = model(data)\n",
    "            loss = nn.MSELoss()(output, target)\n",
    "            if loss_ema < 0:\n",
    "                loss_ema = loss.item()\n",
    "            loss_ema = loss_ema * 0.99 + loss.item()*0.01\n",
    "    print(\"Test loss: {}\".format(loss_ema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dimension\n",
    "input_dim = 76\n",
    "hidden_dim = 60  # hidden layer dimension\n",
    "layer_dim = 1   # number of hidden layers\n",
    "output_dim = 60   # output dimension\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"  \n",
    "    \n",
    "learning_rate = 0.0001\n",
    "momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea01ac9fdbee446fb0951b737fc7dd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-db4b405d3857>:7: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  out = torch.LongTensor(out)\n",
      "<ipython-input-39-db4b405d3857>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(out, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "net = NN(input_dim, output_dim, hidden_dim, layer_dim)\n",
    "model = net.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum, weight_decay=1e-5)\n",
    "num_epoch = 10\n",
    "losses = []\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    loss = train(model, device, train_loader, optimizer, epoch)\n",
    "    losses.append(loss)\n",
    "    test(model, device, val_loader)\n",
    "    torch.save(model.state_dict(), 'checkpoints/train-epoch-gru-2linear-{}.pth'.format(epoch + 1)) \n",
    "    \n",
    "with open(\"losses_test.txt\", \"w\") as output:\n",
    "    output.write(str(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"checkpoints/train-epoch-lstm-ad5.pth\",map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4\n",
    "\n",
    "def my_test_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "#     print(batch)\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    inp = torch.tensor(inp, dtype=torch.float)\n",
    "    agent = [scene['agent_id'] for scene in batch]\n",
    "    track = [scene['track_id'] for scene in batch]\n",
    "    track_ids = []\n",
    "    for scene in range(len(agent)):\n",
    "        for track_id in range(len(track[scene])):\n",
    "            if agent[scene] == track[scene][track_id][0][0]:\n",
    "                track_ids.append(track_id)\n",
    "                break\n",
    "    return [inp, track_ids]\n",
    "\n",
    "# intialize a dataset\n",
    "val_dataset  = ArgoverseDataset(data_path=val_path)\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_test_collate, num_workers=0, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, agent in test_loader:\n",
    "            data = torch.reshape(data, (4, 60, -1)).to(device)\n",
    "            output = torch.reshape(model(data), (4, 60, 30, -1))\n",
    "            for i in range(len(agent)):\n",
    "                scene = output[i][agent[i]]\n",
    "                predictions.append(scene)\n",
    "    predict = [torch.reshape(t, (-1,)) for t in predictions]\n",
    "    sample = pd.read_csv('sample_submission.csv')\n",
    "    preds_df = sample.set_index('ID')\n",
    "    for i in range(3200):\n",
    "        preds_df.iloc[i] = predict[i].tolist()\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN(input_dim, output_dim, hidden_dim, layer_dim)\n",
    "net.load_state_dict(torch.load(\"checkpoints/train-epoch-lstm7.pth\",map_location=device))\n",
    "model = net.to(device)\n",
    "preds = predict(model, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv('test_preds_lstmad_5epoch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>373.583618</td>\n",
       "      <td>409.271149</td>\n",
       "      <td>373.070190</td>\n",
       "      <td>409.814087</td>\n",
       "      <td>373.118713</td>\n",
       "      <td>409.852936</td>\n",
       "      <td>373.660858</td>\n",
       "      <td>410.041870</td>\n",
       "      <td>373.374847</td>\n",
       "      <td>409.721344</td>\n",
       "      <td>...</td>\n",
       "      <td>373.421082</td>\n",
       "      <td>409.322083</td>\n",
       "      <td>373.111237</td>\n",
       "      <td>410.563293</td>\n",
       "      <td>373.395081</td>\n",
       "      <td>409.945557</td>\n",
       "      <td>372.836731</td>\n",
       "      <td>409.021606</td>\n",
       "      <td>373.423096</td>\n",
       "      <td>408.717377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>307.479889</td>\n",
       "      <td>594.395691</td>\n",
       "      <td>306.787903</td>\n",
       "      <td>595.172546</td>\n",
       "      <td>306.849792</td>\n",
       "      <td>595.035889</td>\n",
       "      <td>306.841583</td>\n",
       "      <td>595.468994</td>\n",
       "      <td>307.370422</td>\n",
       "      <td>594.602295</td>\n",
       "      <td>...</td>\n",
       "      <td>307.010834</td>\n",
       "      <td>594.812439</td>\n",
       "      <td>306.920563</td>\n",
       "      <td>594.825012</td>\n",
       "      <td>307.250366</td>\n",
       "      <td>595.341003</td>\n",
       "      <td>307.315063</td>\n",
       "      <td>594.517883</td>\n",
       "      <td>306.822601</td>\n",
       "      <td>594.301147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>307.479889</td>\n",
       "      <td>594.395691</td>\n",
       "      <td>306.787903</td>\n",
       "      <td>595.172546</td>\n",
       "      <td>306.849792</td>\n",
       "      <td>595.035889</td>\n",
       "      <td>306.841583</td>\n",
       "      <td>595.468994</td>\n",
       "      <td>307.370422</td>\n",
       "      <td>594.602295</td>\n",
       "      <td>...</td>\n",
       "      <td>307.010834</td>\n",
       "      <td>594.812439</td>\n",
       "      <td>306.920563</td>\n",
       "      <td>594.825012</td>\n",
       "      <td>307.250366</td>\n",
       "      <td>595.341003</td>\n",
       "      <td>307.315063</td>\n",
       "      <td>594.517883</td>\n",
       "      <td>306.822601</td>\n",
       "      <td>594.301147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10028</th>\n",
       "      <td>365.634521</td>\n",
       "      <td>397.756958</td>\n",
       "      <td>365.154144</td>\n",
       "      <td>398.212952</td>\n",
       "      <td>365.262207</td>\n",
       "      <td>398.203186</td>\n",
       "      <td>365.680573</td>\n",
       "      <td>398.384827</td>\n",
       "      <td>365.398865</td>\n",
       "      <td>398.197174</td>\n",
       "      <td>...</td>\n",
       "      <td>365.546722</td>\n",
       "      <td>397.687622</td>\n",
       "      <td>365.194733</td>\n",
       "      <td>398.930786</td>\n",
       "      <td>365.552155</td>\n",
       "      <td>398.312927</td>\n",
       "      <td>364.974731</td>\n",
       "      <td>397.506287</td>\n",
       "      <td>365.509460</td>\n",
       "      <td>397.180695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>414.335846</td>\n",
       "      <td>468.951721</td>\n",
       "      <td>414.237823</td>\n",
       "      <td>469.793640</td>\n",
       "      <td>413.883850</td>\n",
       "      <td>469.741394</td>\n",
       "      <td>414.674194</td>\n",
       "      <td>470.003113</td>\n",
       "      <td>414.295105</td>\n",
       "      <td>469.289734</td>\n",
       "      <td>...</td>\n",
       "      <td>414.399597</td>\n",
       "      <td>469.401367</td>\n",
       "      <td>413.973083</td>\n",
       "      <td>470.124146</td>\n",
       "      <td>414.219147</td>\n",
       "      <td>469.857361</td>\n",
       "      <td>413.987030</td>\n",
       "      <td>468.898010</td>\n",
       "      <td>414.446289</td>\n",
       "      <td>468.523499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               v1          v2          v3          v4          v5          v6  \\\n",
       "ID                                                                              \n",
       "10002  373.583618  409.271149  373.070190  409.814087  373.118713  409.852936   \n",
       "10015  307.479889  594.395691  306.787903  595.172546  306.849792  595.035889   \n",
       "10019  307.479889  594.395691  306.787903  595.172546  306.849792  595.035889   \n",
       "10028  365.634521  397.756958  365.154144  398.212952  365.262207  398.203186   \n",
       "1003   414.335846  468.951721  414.237823  469.793640  413.883850  469.741394   \n",
       "\n",
       "               v7          v8          v9         v10  ...         v51  \\\n",
       "ID                                                     ...               \n",
       "10002  373.660858  410.041870  373.374847  409.721344  ...  373.421082   \n",
       "10015  306.841583  595.468994  307.370422  594.602295  ...  307.010834   \n",
       "10019  306.841583  595.468994  307.370422  594.602295  ...  307.010834   \n",
       "10028  365.680573  398.384827  365.398865  398.197174  ...  365.546722   \n",
       "1003   414.674194  470.003113  414.295105  469.289734  ...  414.399597   \n",
       "\n",
       "              v52         v53         v54         v55         v56         v57  \\\n",
       "ID                                                                              \n",
       "10002  409.322083  373.111237  410.563293  373.395081  409.945557  372.836731   \n",
       "10015  594.812439  306.920563  594.825012  307.250366  595.341003  307.315063   \n",
       "10019  594.812439  306.920563  594.825012  307.250366  595.341003  307.315063   \n",
       "10028  397.687622  365.194733  398.930786  365.552155  398.312927  364.974731   \n",
       "1003   469.401367  413.973083  470.124146  414.219147  469.857361  413.987030   \n",
       "\n",
       "              v58         v59         v60  \n",
       "ID                                         \n",
       "10002  409.021606  373.423096  408.717377  \n",
       "10015  594.517883  306.822601  594.301147  \n",
       "10019  594.517883  306.822601  594.301147  \n",
       "10028  397.506287  365.509460  397.180695  \n",
       "1003   468.898010  414.446289  468.523499  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
