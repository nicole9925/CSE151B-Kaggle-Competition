{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"./new_train/new_train\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "val_dataset  = ArgoverseDataset(data_path=new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_list=glob(os.path.join(new_path, '*'))\n",
    "# pkl_path = pkl_list[0]\n",
    "# with open(pkl_path, 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    out = [numpy.dstack([scene['p_out'], scene['v_out']]) for scene in batch]\n",
    "    inp = torch.LongTensor(inp)\n",
    "    out = torch.LongTensor(out)\n",
    "    return [inp, out]\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(60*19*4, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 48)\n",
    "        self.fc4 = nn.Linear(48, 16)\n",
    "        self.fc5 = nn.Linear(16, 60*30*4)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "net = Model()\n",
    "#net(x) is the same as net.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "print(sum(p.numel() for p in net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e34a6e23e746>:7: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  inp = torch.LongTensor(inp)\n",
      "<ipython-input-3-e34a6e23e746>:8: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  out = torch.LongTensor(out)\n",
      "<ipython-input-104-3612a50b1d4d>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float)\n",
      "<ipython-input-104-3612a50b1d4d>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 17878.972\n",
      "[1,   200] loss: 17560.708\n",
      "[1,   300] loss: 19011.515\n",
      "[1,   400] loss: 14556.279\n",
      "[1,   500] loss: 16279.071\n",
      "[1,   600] loss: 14411.552\n",
      "[1,   700] loss: 14721.864\n",
      "[1,   800] loss: 16019.952\n",
      "[1,   900] loss: 20141.909\n",
      "[1,  1000] loss: 16329.548\n",
      "[1,  1100] loss: 17104.844\n",
      "[1,  1200] loss: 17198.701\n",
      "[1,  1300] loss: 18059.247\n",
      "[1,  1400] loss: 16198.881\n",
      "[1,  1500] loss: 16178.670\n",
      "[1,  1600] loss: 15650.260\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "device = \"cuda\"\n",
    "net = Model().to(device)\n",
    "print(sum(p.numel() for p in net.parameters()))\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data #input has shape BxCxHxW and labels has shape B, print(inputs.shape) is a very useful function\n",
    "        \n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float)\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        for mset, mlabel in zip(inputs, labels):\n",
    "            outputs = net(mset.cuda()) #net.forward(inputs)\n",
    "            outputs = torch.reshape(outputs, (60, 30, 4))\n",
    "            loss = torch.mean((outputs-mlabel.cuda())**2)  # loss = torch.mean((ouputs - labels)**2)+74\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "tensor([[[[-2.8910e+03, -1.3570e+03, -8.0000e+00, -6.0000e+00],\n",
      "          [-2.8920e+03, -1.3580e+03, -7.0000e+00, -7.0000e+00],\n",
      "          [-2.8920e+03, -1.3590e+03, -6.0000e+00, -7.0000e+00],\n",
      "          ...,\n",
      "          [-2.9130e+03, -1.3770e+03, -7.0000e+00, -7.0000e+00],\n",
      "          [-2.9130e+03, -1.3780e+03, -7.0000e+00, -7.0000e+00],\n",
      "          [-2.9140e+03, -1.3790e+03, -7.0000e+00, -6.0000e+00]],\n",
      "\n",
      "         [[-2.8240e+03, -1.3040e+03,  7.0000e+00,  5.0000e+00],\n",
      "          [-2.8240e+03, -1.3040e+03,  6.0000e+00,  6.0000e+00],\n",
      "          [-2.8230e+03, -1.3030e+03,  5.0000e+00,  5.0000e+00],\n",
      "          ...,\n",
      "          [-2.8040e+03, -1.2860e+03,  9.0000e+00,  6.0000e+00],\n",
      "          [-2.8030e+03, -1.2850e+03,  7.0000e+00,  8.0000e+00],\n",
      "          [-2.8030e+03, -1.2840e+03,  6.0000e+00,  9.0000e+00]],\n",
      "\n",
      "         [[-2.8980e+03, -1.3600e+03, -1.0000e+01, -8.0000e+00],\n",
      "          [-2.8990e+03, -1.3610e+03, -1.0000e+01, -9.0000e+00],\n",
      "          [-2.9000e+03, -1.3620e+03, -9.0000e+00, -9.0000e+00],\n",
      "          ...,\n",
      "          [-2.9260e+03, -1.3860e+03, -8.0000e+00, -9.0000e+00],\n",
      "          [-2.9270e+03, -1.3870e+03, -9.0000e+00, -9.0000e+00],\n",
      "          [-2.9280e+03, -1.3880e+03, -9.0000e+00, -7.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2742e-06,  1.6961e-07,  1.0067e-09,  5.6460e-06],\n",
      "          [ 4.1841e-09,  1.4590e-06,  8.2615e-07,  1.3813e-05],\n",
      "          [ 4.1771e-07,  4.4715e-09,  9.6092e-08,  7.9895e-11],\n",
      "          ...,\n",
      "          [ 3.1957e-06,  1.5684e-03,  2.6661e-11,  1.0359e-05],\n",
      "          [ 4.7312e-07,  2.9415e-06,  6.2007e-10,  5.1036e-04],\n",
      "          [ 6.7531e-05,  2.0501e-05,  1.7779e-06,  2.7710e-11]],\n",
      "\n",
      "         [[ 6.0846e-06,  4.1852e-06,  1.3495e-08,  2.5368e-06],\n",
      "          [ 2.2044e-07,  2.9259e-11,  1.2981e-05,  6.3472e-07],\n",
      "          [ 1.0797e-06,  1.2363e-09,  2.4951e-07,  1.9064e-11],\n",
      "          ...,\n",
      "          [ 2.3674e-07,  6.2336e-08,  1.5566e-08,  6.6942e-08],\n",
      "          [ 5.8874e-12,  2.6297e-10,  1.6400e-07,  9.5765e-11],\n",
      "          [ 3.4569e-10,  2.1736e-09,  1.7360e-05,  1.3444e-07]],\n",
      "\n",
      "         [[ 5.2176e-06,  2.2561e-08,  9.3575e-08,  2.7522e-07],\n",
      "          [ 3.2947e-09,  3.3079e-09,  7.3159e-08,  2.1435e-04],\n",
      "          [ 2.9717e-07,  8.7882e-09,  9.8539e-08,  3.0632e-08],\n",
      "          ...,\n",
      "          [ 1.4164e-09,  1.7724e-07,  1.7599e-05,  1.6968e-11],\n",
      "          [ 4.1956e-10,  5.4568e-09,  5.1987e-07,  6.2880e-07],\n",
      "          [ 2.2345e-12,  5.3054e-05,  1.1016e-11,  1.7996e-05]]],\n",
      "\n",
      "\n",
      "        [[[-7.3200e+02, -1.7070e+03,  7.7528e-11,  1.0000e+00],\n",
      "          [-7.3200e+02, -1.7070e+03,  0.0000e+00,  7.6317e-12],\n",
      "          [-7.3200e+02, -1.7070e+03,  1.0000e+00,  1.8453e-08],\n",
      "          ...,\n",
      "          [-7.3200e+02, -1.7070e+03,  1.1688e-23,  2.1202e-24],\n",
      "          [-7.3200e+02, -1.7070e+03,  5.7906e-07,  0.0000e+00],\n",
      "          [-7.3200e+02, -1.7070e+03,  5.0598e-11,  1.0000e+00]],\n",
      "\n",
      "         [[-7.3600e+02, -1.6910e+03, -2.0000e+00,  4.0000e+00],\n",
      "          [-7.3700e+02, -1.6900e+03, -5.0000e+00,  9.0000e+00],\n",
      "          [-7.3700e+02, -1.6900e+03,  0.0000e+00,  4.0000e+00],\n",
      "          ...,\n",
      "          [-7.4300e+02, -1.6750e+03, -4.0000e+00,  5.0000e+00],\n",
      "          [-7.4300e+02, -1.6740e+03, -3.0000e+00,  5.0000e+00],\n",
      "          [-7.4300e+02, -1.6740e+03,  4.1002e-28,  1.0000e+00]],\n",
      "\n",
      "         [[-7.3100e+02, -1.6960e+03,  1.4083e-09,  1.0000e+00],\n",
      "          [-7.3100e+02, -1.6960e+03,  3.9651e-19,  0.0000e+00],\n",
      "          [-7.3100e+02, -1.6960e+03,  1.0000e+00,  0.0000e+00],\n",
      "          ...,\n",
      "          [-7.3100e+02, -1.6960e+03,  1.0000e+00,  1.0034e-11],\n",
      "          [-7.3100e+02, -1.6960e+03,  1.0364e-38,  1.8477e-13],\n",
      "          [-7.3100e+02, -1.6960e+03,  3.0806e-30,  1.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.1967e-07,  1.6606e-06,  2.8590e-09,  2.4601e-07],\n",
      "          [ 4.3725e-09,  7.8530e-07,  2.0091e-08,  4.2054e-06],\n",
      "          [ 7.7685e-07,  8.7821e-08,  4.3337e-09,  6.1013e-09],\n",
      "          ...,\n",
      "          [ 7.4553e-07,  9.3903e-05,  1.1013e-13,  2.7416e-07],\n",
      "          [ 1.7034e-06,  5.7906e-06,  2.8887e-07,  6.6224e-06],\n",
      "          [ 7.9995e-07,  7.8279e-08,  2.2760e-07,  4.1848e-09]],\n",
      "\n",
      "         [[ 3.3055e-06,  5.8927e-06,  2.6573e-06,  5.8944e-08],\n",
      "          [ 1.9368e-07,  1.5623e-09,  4.0453e-05,  6.2190e-06],\n",
      "          [ 2.9582e-05,  1.0808e-07,  3.8292e-07,  3.6418e-10],\n",
      "          ...,\n",
      "          [ 1.7975e-08,  3.7275e-07,  4.6441e-07,  3.1026e-06],\n",
      "          [ 1.2503e-10,  2.7222e-09,  4.7962e-08,  6.3619e-11],\n",
      "          [ 1.4979e-07,  9.3476e-09,  4.3467e-05,  2.1230e-07]],\n",
      "\n",
      "         [[ 3.5498e-07,  5.2815e-09,  8.1465e-06,  3.9273e-07],\n",
      "          [ 8.4673e-09,  3.8423e-07,  2.4946e-06,  1.2666e-08],\n",
      "          [ 3.5919e-09,  1.8162e-10,  3.9683e-08,  1.6963e-07],\n",
      "          ...,\n",
      "          [ 1.3465e-09,  3.5778e-06,  1.6182e-05,  1.1332e-08],\n",
      "          [ 2.4868e-07,  1.5543e-07,  1.7430e-06,  1.6781e-06],\n",
      "          [ 3.3276e-09,  1.0744e-05,  2.5604e-12,  7.4772e-05]]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-117-630361ec44c3>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inp = torch.tensor(inp, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# I think inp is p_x, p_y, v_x, v_y for 60 objects over 19 timestamps\n",
    "# inp = torch.reshape(inp, (4,60*19*4))\n",
    "inp = torch.reshape(inp, (2,60*19*4))\n",
    "inp = torch.tensor(inp, dtype=torch.float)\n",
    "# Out is the same but for 30r timestamps\n",
    "predict = net.predict(inp)\n",
    "predict = torch.reshape(predict, (2,60,30,4))\n",
    "print(predict-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
